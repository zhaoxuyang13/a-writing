\section{Conclusion}

This paper describe a novel algorithm MAML, Model-Agnostic Meta Learning, offers a general approach to adapt meta-learning for most ML algorithms based on gradient descent. 
This methods has a lot of benefits. Firstly, it a simple and easy to understand approach to take, because it doesn't introduce any learned parameters for meta-learning. So that it can be combined with any model representation that is amenable to gradient-based training. Our case study shows that MAML can be applied to few-shot supervised learning problems like classification, regression, as well as few-shot reinforcement learn tasks. What's more, MAML is typically meta-learning the initial parameter (or a weight initialization), adaptation can be performed no matter what amount of data and what number of gradient steps. Simply applying MAML to few-shot image classification problems can already achieve state-of-the-art performance, when the original paper was published. 

\textbf{Future Work} 
Reusing knowledge from past tasks may be a crucial ingredient in making high-capacity scalable models, such as deep neural networks. 
Meta-learning is a huge field while MAML only solved the few-shot learning problems by meta-learning the initial parameters. Other metrics including robustness\cite{Lee2021MachineLR,Simester2020TargetingPC,HancoxLi2020RobustnessIM,Sehwag2019AnalyzingTR,Chandrasekaran2019LG} of ML can also be meta-learned for other cases, while other meta-learning approach like metric-based \cite{Chen2020VariationalMS, Wang2021MetricbasedMM,Wang2020FewShotRB},model-based\cite{Belkhale2020ModelBasedMF,Xu2020FastTA,CoReyes2021ACCELERATINGOR } can also be taken. With the thriving of ML academic research, learning techniques and concepts like long short-term memory\cite{Sak2014LongSM}, attention\cite{Martin2019InterpretableML}, neural networks\cite{Schmidhuber2015DeepLI,Abadi2016TensorFlowAS}, architecture search\cite{Jafra2018ARO} also bring opportunities to new meta-learning algorithms.
With opportunities also comes open problems. For example, diverse and multi-modal task distributions.Many big successes of meta-learning have been within narrow task families, while learning on diverse task distributions can challenge existing methods. How to learn more general knowledge just like human beings remain an interesting while challenging open problem for both computer scientists, neural scientists\cite{Horton1984HumanLA,Lindsay2020AttentionIP}, philosopher\cite{Cowan2012MemoryAH}and anthropologist. Let's hope meta-learning can bring mankind new understanding of themselves and their existence.


